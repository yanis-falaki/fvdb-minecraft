{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fvdb\n",
    "from fvdb.nn import VDBTensor\n",
    "import torch\n",
    "from UNet import SparseUNet\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from utils import setup_plots, update_plots\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# My gpu specifically doesn't support this, if you have an RTX 3000 series or above,\n",
    "# or a GPU architecture equal to or newer than Ampere you can comment/remove this.\n",
    "fvdb.nn.SparseConv3d.allow_tf32 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ChunkDataset\n",
    "This dataset definition loads minecraft chunk data stored as nvdb files into VDBTensors.\n",
    "\n",
    "Each voxel/block contains one value which is it's offset into a global block table, stored at minecraft-serialization/block_list.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkDataset(Dataset):\n",
    "    def __init__(self, chunksPath):\n",
    "        self.paths = []\n",
    "\n",
    "        for filename in os.listdir(chunksPath):\n",
    "            self.paths.append(f\"{chunksPath}/{filename}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        grid_batch, labels, names = fvdb.load(self.paths[idx])\n",
    "        labels.to(torch.long)\n",
    "\n",
    "        return VDBTensor(grid_batch, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChunkDataset(\"data/training_data/chunks\")\n",
    "\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size   # 20% for validation\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "In this example, we're taking the voxel grid and occupancy as an input, and attempting to predict each block offset value in the grid.\n",
    "\n",
    "I.e. we are predicting what minecraft block each voxel is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Object Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, collate_fn=fvdb.jcat, shuffle=True, prefetch_factor=16, num_workers=6)\n",
    "\n",
    "num_classes = 0\n",
    "with open(\"minecraft-serialization/block_list.txt\", 'r') as file:\n",
    "    num_classes = sum(1 for line in file)\n",
    "\n",
    "lr = 1e-4\n",
    "scheduler_step_size = 500\n",
    "scheduler_gamma = 0.1\n",
    "\n",
    "model = SparseUNet(num_classes).to('cuda')\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, scheduler_step_size, scheduler_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'setup_plots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ax1, ax2, loss_line, acc_line \u001b[38;5;241m=\u001b[39m \u001b[43msetup_plots\u001b[49m()\n\u001b[1;32m      3\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'setup_plots' is not defined"
     ]
    }
   ],
   "source": [
    "ax1, ax2, loss_line, acc_line = setup_plots()\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "iterations = []\n",
    "max_epochs = 1\n",
    "max_steps = 1250\n",
    "update_every = 10\n",
    "\n",
    "# Training Loop\n",
    "model.train()\n",
    "steps_passed = 0\n",
    "for epoch in range(max_epochs):\n",
    "    for i, vdb_tensor in enumerate(train_loader):\n",
    "        vdb_tensor = vdb_tensor.cuda()\n",
    "        target = vdb_tensor.data.jdata.squeeze().to(torch.long)\n",
    "        actives = vdb_tensor.grid.jagged_like(torch.ones(target.shape[0], device='cuda', dtype=torch.float32).unsqueeze(1))\n",
    "        X = VDBTensor(grid=vdb_tensor.grid, data=actives)\n",
    "\n",
    "        # forward and backward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(X)\n",
    "        l = loss(y_hat.data.jdata, target)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # step scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        if steps_passed % update_every == 0:\n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(y_hat.data.jdata, dim=1)\n",
    "            acc = (predictions == target).float().mean()\n",
    "\n",
    "            losses.append(l.item())\n",
    "            accuracies.append(acc.item())\n",
    "            iterations.append(steps_passed)\n",
    "            update_plots(losses, accuracies, iterations, ax1, ax2, loss_line, acc_line)\n",
    "\n",
    "        # increment total steps passed\n",
    "        steps_passed += 1\n",
    "        if steps_passed >= max_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, collate_fn=fvdb.jcat, shuffle=True, prefetch_factor=16, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fab8c7d97f468e8d3eebae8bb46f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation accuracy: 0.603\n"
     ]
    }
   ],
   "source": [
    "accuracy_sum = 0\n",
    "update_every = 10\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad(), tqdm(total=len(val_loader)) as pbar:\n",
    "    for i, vdb_tensor in enumerate(val_loader):\n",
    "        vdb_tensor = vdb_tensor.cuda()\n",
    "        target = vdb_tensor.data.jdata.squeeze().to(torch.long)\n",
    "        actives = vdb_tensor.grid.jagged_like(torch.ones(target.shape[0], device='cuda', dtype=torch.float32).unsqueeze(1))\n",
    "\n",
    "        X = VDBTensor(grid=vdb_tensor.grid, data=actives)\n",
    "        y_hat = model(X)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = torch.argmax(y_hat.data.jdata, dim=1)\n",
    "        accuracy_sum += (predictions == target).float().mean()\n",
    "\n",
    "        if (i + 1) % update_every == 0 or (i + 1) == len(val_loader):\n",
    "            current_mean = accuracy_sum / (i + 1)\n",
    "            pbar.set_description(f'Validating (Acc: {current_mean:.3f})')\n",
    "            pbar.update(update_every if i + 1 < len(val_loader) else len(val_loader) % update_every)\n",
    "\n",
    "mean_accuracy = accuracy_sum/len(val_loader)\n",
    "print(f'Final validation accuracy: {mean_accuracy:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fvdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
